{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d8a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import feedparser\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import concurrent.futures\n",
    "import yfinance as yf\n",
    "from urllib.parse import quote\n",
    "import pickle\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41121a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'justetf.csv'\n",
    "etf_data = pd.read_csv(file_path)\n",
    "etf_data\n",
    "etf_data = etf_data.dropna(subset=['ytdReturnCUR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b0986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'),\n",
       " 949         0\n",
       " 1761        0\n",
       " 2079        0\n",
       " 1765        0\n",
       " 208         0\n",
       "         ...  \n",
       " 686     12892\n",
       " 1104    14753\n",
       " 1832    26091\n",
       " 1537    43427\n",
       " 1824    48857\n",
       " Name: fundSizeMillions, Length: 2233, dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'fundSizeMillions' in etf_data.columns:\n",
    "    # Remove commas and convert to numeric values\n",
    "    etf_data['fundSizeMillions'] = pd.to_numeric(\n",
    "        etf_data['fundSizeMillions'].replace(',', '', regex=True), errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "# Verify the data types after conversion and inspect the first few rows\n",
    "etf_data.dtypes['fundSizeMillions'], etf_data['fundSizeMillions'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedfb45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Emerging Markets': 91, 'Developed Markets': 774}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of countries for each group\n",
    "emerging_markets_countries = [\n",
    "    'China', 'India', 'Brazil', 'Indonesia', 'Mexico', 'Russia', 'Argentina',\n",
    "    'South Africa', 'Philippines', 'Saudi Arabia', 'Poland', 'Taiwan', 'Thailand',\n",
    "    'Malaysia', 'Colombia', 'South Korea', 'Turkey'\n",
    "]\n",
    "\n",
    "developed_markets_countries = [\n",
    "    'Canada', 'France', 'Germany', 'Italy', 'Japan', 'United Kingdom', 'United States'\n",
    "]\n",
    "\n",
    "# Function to calculate combined exposure for a list of countries\n",
    "def calculate_combined_exposure(df, countries):\n",
    "    combined_exposure = sum([df[f'exposureCountry_{country}'] for country in countries if f'exposureCountry_{country}' in df.columns])\n",
    "    return combined_exposure\n",
    "\n",
    "# Calculate combined exposures\n",
    "etf_data['combinedEmergingMarkets'] = calculate_combined_exposure(etf_data, emerging_markets_countries)\n",
    "etf_data['combinedDevelopedMarkets'] = calculate_combined_exposure(etf_data, developed_markets_countries)\n",
    "\n",
    "# Set a threshold for significant combined exposure\n",
    "combined_threshold = 0.5\n",
    "fund_size_threshold = 50  # in millions\n",
    "\n",
    "# Filter ETFs by combined exposure\n",
    "etfs_emerging_markets = etf_data[\n",
    "    (etf_data['combinedEmergingMarkets'] >= combined_threshold) &\n",
    "    (etf_data['fundSizeMillions'] >= fund_size_threshold)\n",
    "]\n",
    "\n",
    "etfs_developed_markets = etf_data[\n",
    "    (etf_data['combinedDevelopedMarkets'] >= combined_threshold) &\n",
    "    (etf_data['fundSizeMillions'] >= fund_size_threshold)\n",
    "]\n",
    "\n",
    "# Display the count of ETFs for each group\n",
    "{\n",
    "    \"Emerging Markets\": etfs_emerging_markets.shape[0],\n",
    "    \"Developed Markets\": etfs_developed_markets.shape[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feaee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define features to exclude\n",
    "exclude_columns = ['isin', 'wkn', 'name', 'fundProvider', 'legalStructure', 'ytdReturnCUR']\n",
    "# Select numeric columns and exclude non-relevant features\n",
    "numeric_columns = etf_data.select_dtypes(include=np.number).columns.tolist()\n",
    "feature_columns = [col for col in numeric_columns if col not in exclude_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4446a34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 104), (19, 104), (72,), (19,), (619, 104), (155, 104), (619,), (155,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the relevant data\n",
    "X_emerging = etfs_emerging_markets[feature_columns].fillna(0)\n",
    "y_emerging = etfs_emerging_markets['ytdReturnCUR']\n",
    "\n",
    "X_developed = etfs_developed_markets[feature_columns].fillna(0)\n",
    "y_developed = etfs_developed_markets['ytdReturnCUR']\n",
    "\n",
    "# Split each dataset into train and test sets\n",
    "X_train_emerging, X_test_emerging, y_train_emerging, y_test_emerging = train_test_split(X_emerging, y_emerging, test_size=0.2, random_state=42)\n",
    "X_train_developed, X_test_developed, y_train_developed, y_test_developed = train_test_split(X_developed, y_developed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler_emerging = StandardScaler()\n",
    "scaler_developed = StandardScaler()\n",
    "\n",
    "X_train_emerging_scaled = scaler_emerging.fit_transform(X_train_emerging)\n",
    "X_test_emerging_scaled = scaler_emerging.transform(X_test_emerging)\n",
    "\n",
    "X_train_developed_scaled = scaler_developed.fit_transform(X_train_developed)\n",
    "X_test_developed_scaled = scaler_developed.transform(X_test_developed)\n",
    "\n",
    "# Check the data shapes for both markets\n",
    "(X_train_emerging_scaled.shape, X_test_emerging_scaled.shape, y_train_emerging.shape, y_test_emerging.shape,\n",
    " X_train_developed_scaled.shape, X_test_developed_scaled.shape, y_train_developed.shape, y_test_developed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647179e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013135818640021424,\n",
       " 0.08058535451859156,\n",
       " 0.00828440155728867,\n",
       " 0.06163909017369867)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Function to build and train an MLP model using scikit-learn\n",
    "def train_mlp_model(X_train, y_train, X_test, y_test):\n",
    "    model = MLPRegressor(hidden_layer_sizes=(64, 32, 16), activation='relu', max_iter=500, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    return model, mse, mae\n",
    "\n",
    "# Train the MLP models for emerging and developed markets\n",
    "model_emerging, mse_emerging, mae_emerging = train_mlp_model(X_train_emerging_scaled, y_train_emerging, X_test_emerging_scaled, y_test_emerging)\n",
    "model_developed, mse_developed, mae_developed = train_mlp_model(X_train_developed_scaled, y_train_developed, X_test_developed_scaled, y_test_developed)\n",
    "\n",
    "mse_emerging, mae_emerging, mse_developed, mae_developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21540e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.258948752051156, -4.322152879624173)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predict on the test datasets\n",
    "y_pred_emerging = model_emerging.predict(X_test_emerging_scaled)\n",
    "y_pred_developed = model_developed.predict(X_test_developed_scaled)\n",
    "\n",
    "# Calculate R² scores\n",
    "r2_emerging = r2_score(y_test_emerging, y_pred_emerging)\n",
    "r2_developed = r2_score(y_test_developed, y_pred_developed)\n",
    "\n",
    "r2_emerging, r2_developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6912ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:16<00:00, 21.64it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:18<00:00, 20.20it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:19<00:00, 18.99it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:18<00:00, 20.28it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:17<00:00, 21.18it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:16<00:00, 22.37it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:18<00:00, 19.76it/s]\n",
      "100%|█████████████████████████████████████████| 365/365 [00:20<00:00, 17.45it/s]\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to encode strings to URL-friendly format\n",
    "def encode_string_to_url(input_string):\n",
    "    return quote(input_string)\n",
    "\n",
    "# Construct an RSS URL with economic search terms\n",
    "def construct_rss_url(after, before, search_terms):\n",
    "    search_term = '%20OR%20'.join([encode_string_to_url(term) for term in search_terms])\n",
    "    endpoint = f'https://news.google.com/rss/search?q={search_term}+after:{after}+before:{before}&ceid=US:en&hl=en-US&gl=US'\n",
    "    return endpoint\n",
    "\n",
    "# Function to parse an RSS feed URL and return the articles\n",
    "def parse_rss_feed(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    if feed.bozo == 0:\n",
    "        return [entry.title + \" \" + entry.summary for entry in feed.entries]\n",
    "    else:\n",
    "        raise Exception('Failed feed pull')\n",
    "\n",
    "# Function to process a specific date range and calculate sentiment\n",
    "def process_date(date, search_terms):\n",
    "    after = date.strftime('%Y-%m-%d')\n",
    "    before = (date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    url = construct_rss_url(after, before, search_terms)\n",
    "    articles = parse_rss_feed(url)\n",
    "    \n",
    "    # Calculate sentiment scores\n",
    "    sentiments = [sia.polarity_scores(article) for article in articles]\n",
    "    avg_sentiment = sum([s['compound'] for s in sentiments]) / len(sentiments) if sentiments else 0\n",
    "    return len(articles), avg_sentiment\n",
    "\n",
    "# Load existing data if available\n",
    "def load_existing_data(pickle_file, default_data):\n",
    "    if os.path.exists(pickle_file):\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return default_data\n",
    "\n",
    "# Save data incrementally\n",
    "def save_data(pickle_file, data):\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "search_terms_dict = {\n",
    "    \"Emerging\": {country: [f'{country} economy', f'{country} GDP', 'economic outlook'] for country in emerging_markets_countries},\n",
    "    \"Developed\": {country: [f'{country} economy', f'{country} GDP', 'economic outlook'] for country in developed_markets_countries}\n",
    "}\n",
    "\n",
    "# Define date range\n",
    "date_range = pd.date_range(start='2023-01-01', end='2023-12-31')\n",
    "pickle_file = 'news_sentiment_stratified.pkl'\n",
    "default_data = {market: {country: pd.DataFrame(index=date_range, columns=['article_count', 'avg_sentiment'])\n",
    "                         for country in search_terms_dict[market].keys()}\n",
    "                for market in search_terms_dict.keys()}\n",
    "\n",
    "# Load existing or initialize new data\n",
    "data = load_existing_data(pickle_file, default_data)\n",
    "\n",
    "# Process news data in parallel for each market and country\n",
    "for market in search_terms_dict.keys():\n",
    "    for country, search_terms in search_terms_dict[market].items():\n",
    "        df = data[market][country]\n",
    "        \n",
    "        # Identify dates that still need processing\n",
    "        unprocessed_dates = df[df['article_count'].isna()].index\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(lambda d: process_date(d, search_terms), unprocessed_dates), total=len(unprocessed_dates)))\n",
    "        \n",
    "        for i, date in enumerate(unprocessed_dates):\n",
    "            df.at[date, 'article_count'] = results[i][0]\n",
    "            df.at[date, 'avg_sentiment'] = results[i][1]\n",
    "        \n",
    "        # Save progress for each country to the pickle file\n",
    "        save_data(pickle_file, data)\n",
    "        \n",
    "        # Add a sleep interval to prevent rate limiting\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0d35a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_count</th>\n",
       "      <th>avg_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.15202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.02719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>16</td>\n",
       "      <td>0.102087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.0661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.20356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>17</td>\n",
       "      <td>-0.030976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>10</td>\n",
       "      <td>0.00992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>10</td>\n",
       "      <td>0.17868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.03726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.14007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_count avg_sentiment\n",
       "2023-01-01            10      -0.15202\n",
       "2023-01-02            10      -0.02719\n",
       "2023-01-03            16      0.102087\n",
       "2023-01-04            12       -0.0661\n",
       "2023-01-05            10      -0.20356\n",
       "...                  ...           ...\n",
       "2023-12-27            17     -0.030976\n",
       "2023-12-28            10       0.00992\n",
       "2023-12-29            10       0.17868\n",
       "2023-12-30            10      -0.03726\n",
       "2023-12-31            10      -0.14007\n",
       "\n",
       "[365 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d3335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
